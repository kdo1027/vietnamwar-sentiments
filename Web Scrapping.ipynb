{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'key'\n",
    "start = dateutil.parser.parse('2023-01-01').date()  # Start date in YYYY-MM-DD format\n",
    "end = dateutil.parser.parse('2023-12-31').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(date):\n",
    "    base_url = 'https://api.nytimes.com/svc/archive/v1/'\n",
    "    url = base_url + '/' + date[0] + '/' + date[1] + '.json?api-key=' + API_KEY\n",
    "    try:\n",
    "        response = requests.get(url, verify=False).json()\n",
    "    except Exception:\n",
    "        return None\n",
    "    time.sleep(6)\n",
    "    return response\n",
    "\n",
    "def is_valid(article, date):\n",
    "    is_in_range = start <= date <= end\n",
    "    has_headline = type(article['headline']) == dict and 'main' in article['headline'].keys()\n",
    "    has_vietnam = (\n",
    "        'Vietnam' in article['headline']['main'] or\n",
    "        'Vietnam' in article.get('lead_paragraph', '') or\n",
    "        any('Vietnam' in keyword['value'] for keyword in article['keywords'])\n",
    "    )\n",
    "    return is_in_range and has_headline and has_vietnam\n",
    "\n",
    "def parse_response(response):\n",
    "    data = {\n",
    "        'headline': [],\n",
    "        'date': [],\n",
    "        'web_url': [],\n",
    "        'doc_type': [],\n",
    "        'lead_paragraph': [],\n",
    "        'material_type': [],\n",
    "        'author': [],\n",
    "        'section': [],\n",
    "        'subsection': [],\n",
    "        'keywords': [],\n",
    "        'full_text': []\n",
    "    }\n",
    "    \n",
    "    articles = response['response']['docs']\n",
    "    for article in articles:\n",
    "        date = dateutil.parser.parse(article['pub_date']).date()\n",
    "        if is_valid(article, date):\n",
    "            data['date'].append(date)\n",
    "            data['headline'].append(article['headline']['main'])\n",
    "            data['section'].append(article.get('section_name'))\n",
    "            data['lead_paragraph'].append(article.get('lead_paragraph'))\n",
    "            data['web_url'].append(article.get('web_url'))\n",
    "            data['subsection'].append(article.get('subsection_name'))\n",
    "            data['author'].append(article.get('byline', {}).get('original'))\n",
    "            data['doc_type'].append(article['document_type'])\n",
    "            data['material_type'].append(article.get('type_of_material'))\n",
    "            keywords = [keyword['value'] for keyword in article['keywords'] if 'Vietnam' in keyword['value']]\n",
    "            data['keywords'].append(keywords)\n",
    "            data['full_text'].append('')  # Placeholder for full text\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def get_article_text(url):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    service = Service('path_to_chromedriver')  # Update this path to your chromedriver\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        paragraphs = soup.find_all('p')\n",
    "        full_text = ' '.join([para.get_text() for para in paragraphs])\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving text from {url}: {e}\")\n",
    "        full_text = ''\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return full_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
